{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='exemple_texte.txt' mode='r' encoding='cp1252'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Description = open(\"exemple_texte.txt\", \"r\")\n",
    "Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le concept de Machine Learning date du milieu du 20Ã¨me siÃ¨cle. Dans les annÃ©es 1950, le mathÃ©maticien britannique Alan Turing imagine une machine capable dâ€™apprendre, une Â« Learning Machine Â». \n",
      "Au cours des dÃ©cennies suivantes, diffÃ©rentes techniques de Machine Learning ont Ã©tÃ© dÃ©veloppÃ©es pour crÃ©er des algorithmes capables dâ€™apprendre et de sâ€™amÃ©liorer de maniÃ¨re autonome.\n",
      "Deep Learning vs machine learning ? Pas vraiment\n",
      "Parmi ces techniques, on compte les rÃ©seaux de neurones artificiels. Câ€™est sur ces algorithmes que reposent le Deep Learning, mais aussi des technologies comme la reconnaissance dâ€™images ou la vision robotique.\n",
      "Les rÃ©seaux de neurones artificiels sont inspirÃ©s par les neurones du cerveau humain. Ils sont constituÃ©s de plusieurs neurones artificiels connectÃ©s entre eux. Plus le nombre de neurones est Ã©levÃ©, plus le rÃ©seau est  Â« profond Â».\n"
     ]
    }
   ],
   "source": [
    "contenu = Description.read()\n",
    "print(contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Le',\n",
       " 'concept',\n",
       " 'de',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'date',\n",
       " 'du',\n",
       " 'milieu',\n",
       " 'du',\n",
       " '20Ã¨me',\n",
       " 'siÃ¨cle.',\n",
       " 'Dans',\n",
       " 'les',\n",
       " 'annÃ©es',\n",
       " '1950,',\n",
       " 'le',\n",
       " 'mathÃ©maticien',\n",
       " 'britannique',\n",
       " 'Alan',\n",
       " 'Turing',\n",
       " 'imagine',\n",
       " 'une',\n",
       " 'machine',\n",
       " 'capable',\n",
       " 'dâ€™apprendre,',\n",
       " 'une',\n",
       " 'Â«',\n",
       " 'Learning',\n",
       " 'Machine',\n",
       " 'Â».',\n",
       " 'Au',\n",
       " 'cours',\n",
       " 'des',\n",
       " 'dÃ©cennies',\n",
       " 'suivantes,',\n",
       " 'diffÃ©rentes',\n",
       " 'techniques',\n",
       " 'de',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'ont',\n",
       " 'Ã©tÃ©',\n",
       " 'dÃ©veloppÃ©es',\n",
       " 'pour',\n",
       " 'crÃ©er',\n",
       " 'des',\n",
       " 'algorithmes',\n",
       " 'capables',\n",
       " 'dâ€™apprendre',\n",
       " 'et',\n",
       " 'de',\n",
       " 'sâ€™amÃ©liorer',\n",
       " 'de',\n",
       " 'maniÃ¨re',\n",
       " 'autonome.',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'vs',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '?',\n",
       " 'Pas',\n",
       " 'vraiment',\n",
       " 'Parmi',\n",
       " 'ces',\n",
       " 'techniques,',\n",
       " 'on',\n",
       " 'compte',\n",
       " 'les',\n",
       " 'rÃ©seaux',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'artificiels.',\n",
       " 'Câ€™est',\n",
       " 'sur',\n",
       " 'ces',\n",
       " 'algorithmes',\n",
       " 'que',\n",
       " 'reposent',\n",
       " 'le',\n",
       " 'Deep',\n",
       " 'Learning,',\n",
       " 'mais',\n",
       " 'aussi',\n",
       " 'des',\n",
       " 'technologies',\n",
       " 'comme',\n",
       " 'la',\n",
       " 'reconnaissance',\n",
       " 'dâ€™images',\n",
       " 'ou',\n",
       " 'la',\n",
       " 'vision',\n",
       " 'robotique.',\n",
       " 'Les',\n",
       " 'rÃ©seaux',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'sont',\n",
       " 'inspirÃ©s',\n",
       " 'par',\n",
       " 'les',\n",
       " 'neurones',\n",
       " 'du',\n",
       " 'cerveau',\n",
       " 'humain.',\n",
       " 'Ils',\n",
       " 'sont',\n",
       " 'constituÃ©s',\n",
       " 'de',\n",
       " 'plusieurs',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'connectÃ©s',\n",
       " 'entre',\n",
       " 'eux.',\n",
       " 'Plus',\n",
       " 'le',\n",
       " 'nombre',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'est',\n",
       " 'Ã©levÃ©,',\n",
       " 'plus',\n",
       " 'le',\n",
       " 'rÃ©seau',\n",
       " 'est',\n",
       " 'Â«',\n",
       " 'profond',\n",
       " 'Â».']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = contenu.split()\n",
    "token #tokenizer permet de découper en jeton (ex une phrase=> mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayoub/nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-f29e53fbc2b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontenu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayoub/nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.word_tokenize(contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ayoub\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Le',\n",
       " 'concept',\n",
       " 'de',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'date',\n",
       " 'du',\n",
       " 'milieu',\n",
       " 'du',\n",
       " '20Ã¨me',\n",
       " 'siÃ¨cle',\n",
       " '.',\n",
       " 'Dans',\n",
       " 'les',\n",
       " 'annÃ©es',\n",
       " '1950',\n",
       " ',',\n",
       " 'le',\n",
       " 'mathÃ©maticien',\n",
       " 'britannique',\n",
       " 'Alan',\n",
       " 'Turing',\n",
       " 'imagine',\n",
       " 'une',\n",
       " 'machine',\n",
       " 'capable',\n",
       " 'dâ€™apprendre',\n",
       " ',',\n",
       " 'une',\n",
       " 'Â',\n",
       " '«',\n",
       " 'Learning',\n",
       " 'Machine',\n",
       " 'Â',\n",
       " '»',\n",
       " '.',\n",
       " 'Au',\n",
       " 'cours',\n",
       " 'des',\n",
       " 'dÃ©cennies',\n",
       " 'suivantes',\n",
       " ',',\n",
       " 'diffÃ©rentes',\n",
       " 'techniques',\n",
       " 'de',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'ont',\n",
       " 'Ã©tÃ©',\n",
       " 'dÃ©veloppÃ©es',\n",
       " 'pour',\n",
       " 'crÃ©er',\n",
       " 'des',\n",
       " 'algorithmes',\n",
       " 'capables',\n",
       " 'dâ€™apprendre',\n",
       " 'et',\n",
       " 'de',\n",
       " 'sâ€™amÃ©liorer',\n",
       " 'de',\n",
       " 'maniÃ¨re',\n",
       " 'autonome',\n",
       " '.',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'vs',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '?',\n",
       " 'Pas',\n",
       " 'vraiment',\n",
       " 'Parmi',\n",
       " 'ces',\n",
       " 'techniques',\n",
       " ',',\n",
       " 'on',\n",
       " 'compte',\n",
       " 'les',\n",
       " 'rÃ©seaux',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " '.',\n",
       " 'Câ€™est',\n",
       " 'sur',\n",
       " 'ces',\n",
       " 'algorithmes',\n",
       " 'que',\n",
       " 'reposent',\n",
       " 'le',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " ',',\n",
       " 'mais',\n",
       " 'aussi',\n",
       " 'des',\n",
       " 'technologies',\n",
       " 'comme',\n",
       " 'la',\n",
       " 'reconnaissance',\n",
       " 'dâ€™images',\n",
       " 'ou',\n",
       " 'la',\n",
       " 'vision',\n",
       " 'robotique',\n",
       " '.',\n",
       " 'Les',\n",
       " 'rÃ©seaux',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'sont',\n",
       " 'inspirÃ©s',\n",
       " 'par',\n",
       " 'les',\n",
       " 'neurones',\n",
       " 'du',\n",
       " 'cerveau',\n",
       " 'humain',\n",
       " '.',\n",
       " 'Ils',\n",
       " 'sont',\n",
       " 'constituÃ©s',\n",
       " 'de',\n",
       " 'plusieurs',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'connectÃ©s',\n",
       " 'entre',\n",
       " 'eux',\n",
       " '.',\n",
       " 'Plus',\n",
       " 'le',\n",
       " 'nombre',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'est',\n",
       " 'Ã©levÃ©',\n",
       " ',',\n",
       " 'plus',\n",
       " 'le',\n",
       " 'rÃ©seau',\n",
       " 'est',\n",
       " 'Â',\n",
       " '«',\n",
       " 'profond',\n",
       " 'Â',\n",
       " '»',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(contenu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Le',\n",
       " 'concept',\n",
       " 'de',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'date',\n",
       " 'du',\n",
       " 'milieu',\n",
       " 'du',\n",
       " '20Ã',\n",
       " 'me',\n",
       " 'siÃ',\n",
       " 'cle',\n",
       " 'Dans',\n",
       " 'les',\n",
       " 'annÃ',\n",
       " 'es',\n",
       " '1950',\n",
       " 'le',\n",
       " 'mathÃ',\n",
       " 'maticien',\n",
       " 'britannique',\n",
       " 'Alan',\n",
       " 'Turing',\n",
       " 'imagine',\n",
       " 'une',\n",
       " 'machine',\n",
       " 'capable',\n",
       " 'dâ',\n",
       " 'apprendre',\n",
       " 'une',\n",
       " 'Â',\n",
       " 'Learning',\n",
       " 'Machine',\n",
       " 'Â',\n",
       " 'Au',\n",
       " 'cours',\n",
       " 'des',\n",
       " 'dÃ',\n",
       " 'cennies',\n",
       " 'suivantes',\n",
       " 'diffÃ',\n",
       " 'rentes',\n",
       " 'techniques',\n",
       " 'de',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'ont',\n",
       " 'Ã',\n",
       " 'tÃ',\n",
       " 'dÃ',\n",
       " 'veloppÃ',\n",
       " 'es',\n",
       " 'pour',\n",
       " 'crÃ',\n",
       " 'er',\n",
       " 'des',\n",
       " 'algorithmes',\n",
       " 'capables',\n",
       " 'dâ',\n",
       " 'apprendre',\n",
       " 'et',\n",
       " 'de',\n",
       " 'sâ',\n",
       " 'amÃ',\n",
       " 'liorer',\n",
       " 'de',\n",
       " 'maniÃ',\n",
       " 're',\n",
       " 'autonome',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'vs',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'Pas',\n",
       " 'vraiment',\n",
       " 'Parmi',\n",
       " 'ces',\n",
       " 'techniques',\n",
       " 'on',\n",
       " 'compte',\n",
       " 'les',\n",
       " 'rÃ',\n",
       " 'seaux',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'Câ',\n",
       " 'est',\n",
       " 'sur',\n",
       " 'ces',\n",
       " 'algorithmes',\n",
       " 'que',\n",
       " 'reposent',\n",
       " 'le',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'mais',\n",
       " 'aussi',\n",
       " 'des',\n",
       " 'technologies',\n",
       " 'comme',\n",
       " 'la',\n",
       " 'reconnaissance',\n",
       " 'dâ',\n",
       " 'images',\n",
       " 'ou',\n",
       " 'la',\n",
       " 'vision',\n",
       " 'robotique',\n",
       " 'Les',\n",
       " 'rÃ',\n",
       " 'seaux',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'sont',\n",
       " 'inspirÃ',\n",
       " 's',\n",
       " 'par',\n",
       " 'les',\n",
       " 'neurones',\n",
       " 'du',\n",
       " 'cerveau',\n",
       " 'humain',\n",
       " 'Ils',\n",
       " 'sont',\n",
       " 'constituÃ',\n",
       " 's',\n",
       " 'de',\n",
       " 'plusieurs',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'connectÃ',\n",
       " 's',\n",
       " 'entre',\n",
       " 'eux',\n",
       " 'Plus',\n",
       " 'le',\n",
       " 'nombre',\n",
       " 'de',\n",
       " 'neurones',\n",
       " 'est',\n",
       " 'Ã',\n",
       " 'levÃ',\n",
       " 'plus',\n",
       " 'le',\n",
       " 'rÃ',\n",
       " 'seau',\n",
       " 'est',\n",
       " 'Â',\n",
       " 'profond',\n",
       " 'Â']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')#fonction  inject Tokenizer /#pour éliminer les virgules et garder comme token que les mots\n",
    "Sac_de_mots = tokenizer.tokenize(contenu)#on reprend ensuite la fonction tokenize\n",
    "Sac_de_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayoub/nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayoub/nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0b54a6aa9ea3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'french'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\ayoub/nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\ayoub\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('french')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayoub\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au', 'aux', 'avec', 'ce', 'ces', 'dans', 'de', 'des', 'du', 'elle']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('french')\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtre=[]\n",
    "for w in Sac_de_mots[1:]:\n",
    "    if w not in stopwords:\n",
    "        Filtre.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['concept',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'date',\n",
       " 'milieu',\n",
       " '20Ã',\n",
       " 'siÃ',\n",
       " 'cle',\n",
       " 'Dans',\n",
       " 'annÃ',\n",
       " '1950',\n",
       " 'mathÃ',\n",
       " 'maticien',\n",
       " 'britannique',\n",
       " 'Alan',\n",
       " 'Turing',\n",
       " 'imagine',\n",
       " 'machine',\n",
       " 'capable',\n",
       " 'dâ',\n",
       " 'apprendre',\n",
       " 'Â',\n",
       " 'Learning',\n",
       " 'Machine',\n",
       " 'Â',\n",
       " 'Au',\n",
       " 'cours',\n",
       " 'dÃ',\n",
       " 'cennies',\n",
       " 'suivantes',\n",
       " 'diffÃ',\n",
       " 'rentes',\n",
       " 'techniques',\n",
       " 'Machine',\n",
       " 'Learning',\n",
       " 'Ã',\n",
       " 'tÃ',\n",
       " 'dÃ',\n",
       " 'veloppÃ',\n",
       " 'crÃ',\n",
       " 'er',\n",
       " 'algorithmes',\n",
       " 'capables',\n",
       " 'dâ',\n",
       " 'apprendre',\n",
       " 'sâ',\n",
       " 'amÃ',\n",
       " 'liorer',\n",
       " 'maniÃ',\n",
       " 're',\n",
       " 'autonome',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'vs',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'Pas',\n",
       " 'vraiment',\n",
       " 'Parmi',\n",
       " 'techniques',\n",
       " 'compte',\n",
       " 'rÃ',\n",
       " 'seaux',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'Câ',\n",
       " 'algorithmes',\n",
       " 'reposent',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'aussi',\n",
       " 'technologies',\n",
       " 'comme',\n",
       " 'reconnaissance',\n",
       " 'dâ',\n",
       " 'images',\n",
       " 'vision',\n",
       " 'robotique',\n",
       " 'Les',\n",
       " 'rÃ',\n",
       " 'seaux',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'inspirÃ',\n",
       " 'neurones',\n",
       " 'cerveau',\n",
       " 'humain',\n",
       " 'Ils',\n",
       " 'constituÃ',\n",
       " 'plusieurs',\n",
       " 'neurones',\n",
       " 'artificiels',\n",
       " 'connectÃ',\n",
       " 'entre',\n",
       " 'Plus',\n",
       " 'nombre',\n",
       " 'neurones',\n",
       " 'Ã',\n",
       " 'levÃ',\n",
       " 'plus',\n",
       " 'rÃ',\n",
       " 'seau',\n",
       " 'Â',\n",
       " 'profond',\n",
       " 'Â']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filtre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Filtre.count(\"neurones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
